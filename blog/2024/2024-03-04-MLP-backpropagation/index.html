<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>This is the first in a series of blog notes dedicating to summarizing the most fundamental concepts in Machine Learning, Deep Learning, AI, and other scientific topics of my interest. Struggling to comprehend, distinguish, and reminisce these concepts myself sometimes, especially before and during important interviews, I would treasure a lot a source of simple, concise, accurate information to revise for those occasions. With that in mind, I created this blog series including fundamental AI/ML concepts and relating interview questions for those preparing to launch their career in the field, or for those who simply need a refresh before moving on to more complicated concepts. If you fancy to dig deeper into these topics, you can refer to my <a href="https://ottovintola.github.io/blog/" rel="external nofollow noopener" target="_blank">colleague’s blog</a> for a more comprehensive and thorough series.</p> <p>In this first blog, we will go through the concept of Deep Learning, Linear Classifier, Multi-Layer Perception (MLP) and Backpropagation</p> <h2 id="deep-learning---what-is-it">Deep Learning - what is it?</h2> <p><strong>Deep Learning (DL)</strong> is the subset of ML that focuses on the studies of Artificial Neural Networks (ANNs) with feature learning. DL architectures take in large amount of data (structured/unstructured), and use complex algorithms that utilizes multiple layers (thus “deep”) to progressively extract higher-level features and hidden patterns from the raw input. For example, lower layers might identify edges of an image while higher layers identify features that can easily be perceived by human, such as a dog, or a number.</p> <p><strong>Feature learning</strong> (supervised, unsupervised, self-supervised) or representation learning referes to the automatic extraction &amp; selection of relevant features to discover useful representations or patterns <em>directly</em> from raw input. This replaces the manual feature engineering/designing process and allows automatic learning and using the features for specific tasks.</p> <hr> <h2 id="linear-classifier">Linear Classifier</h2> <p><strong>Linear Classifier</strong> is the simplest ML models and thus it can be considered as elementary components for DL models, which is used for binary or multiclass classification tasks. It makes prediction by combining the input features with weights, and the decision boundary is a linear combination of these features. Therefore, its effectiveness depends on the linearity of the underlying patterns.</p> <hr> <h2 id="multi-layer-perceptron">Multi-Layer Perceptron</h2> <p><strong>MLP</strong> is a supplement of feedforward neural network that consists of 3 types of layers: 1 input, 1 or several hidden (because the outputs of this layer are not visible outside the model ), and 1 output layer. Each node in a layer is connected to every node in the next, thus these layers are called <em>fully-connected</em>. MLP can be applied to various tasks, from classification, regression, and pattern recognition</p> <hr> <h2 id="backpropagation">Backpropagation</h2> <p><strong>Backpropagation</strong> is a gradient estimation used to train NN models. The key idea is to iteratively adjust the weights of the NN to reduce the error between the predicted and actual outputs. Back prob consists of <em>two</em> phases/passes:</p> <ul> <li> <p>Forward pass: Input is fed into the NN, and the weighted sum of inputs is calculated at each neuron in each layer. Activation functions are applied to these sums to produce the output of each neuron. Then, the error or loss is calculated by comparing this output to the actual target values.</p> </li> <li> <p>Backward pass: This pass involves the propagation of the error backward through the network to update the weights. The gradient of the loss function with respect to the weights is calculated using the chain rule of calculus. The weights are then adjusted in the opposite direction of the gradient to minimize the error, using done using optimization algorithm such as gradient descent. The updated weights improve the network’s performance, and the process is repeated for multiple epochs until the model converges to a satisfactory solution.</p> </li> </ul> <hr> <h3 id="interview-questions">Interview questions</h3> <ol> <li>Explain the steps involved in the backpropagation algorithm and how it works in training a neural network:</li> </ol> <p><em>Answer: refer to the 2 passes above</em></p> <ol> <li>What is the role of the activation function in the backpropagation algorithm? Examples of commonly used activation functions and explain when to use them?</li> </ol> <p><em>Answer: Activation function introduces non-linearity into the NN, allowing it to learn complex patterns. Common activation functions include:</em> <em>- Rectified Linear Unit (ReLU) for hidden layers due to its simplicity and ability to mitigate the vanishing gradient problem</em></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>*- Sigmoid for inary classification output*

*- Softmax for multi-class classification output to provide probability distribution over the classes*
</code></pre></div></div> <ol> <li>How does the vanishing gradient problem affect the training of deep neural networks, and what strategies can be employed to mitigate this issue in the context of backpropagation?</li> </ol> <p><em>Answer: Vanishing gradient problem occurs when gradients become extremely small during backpropagation as more layers using the same activation functions are added to the network, hindering the update of weights in earlier layers. This is because, by the chain rule, the derivatives of each layer are multiplied together (backward); thus the gradient decreases exponentially as we back-propagate to the initial layers. This is particularly problematic in deep NN.</em></p> <p>*To mitigate this, we could use weight initialization methods such as He for ReLU which doesn’t cause a small derivative. Alternatively, we could use non-saturating activation functions, or implement skip connections (e.g. in Residual Networks). *</p> <ol> <li>Impact of hyperparameters (learning rate, batch size, number of epochs) on the backpropagation algorithm?</li> </ol> <p><em>Answer: Learning rate determines the step size during weight updates - too high rate might lead to overshooting while too low rate leads to slow convergence. Batch size affects the efficiency of weight updates - larger batches provide smoother convergence. The number of epochs influences the duration of training. Setting these hyperparameters involves trade-offs between dataset size, computational resources, and the need for regularization.</em></p> <ol> <li>Explain the concept of weight initialization in the context of backpropagation.</li> </ol> <p><em>Answer: Weight initialization is crucial as it influences the starting point of the optimization process. Poor initialization can lead to slow convergence or getting stuck in local minima. Common techniques include He initialization for ReLU and Xavier/Glorot initialization for Sigmoid or Tanh. These methods set the initial weights that balance the variance of inputs and outputs in each layer, promoting a stable and effective training process</em></p> </body></html>