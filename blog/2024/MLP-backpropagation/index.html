<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Introduction to Deep Learning | Long Nguyen </title> <meta name="author" content="Long Nguyen"> <meta name="description" content="A series of notes for revision purpose"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://normsie.github.io/blog/2024/MLP-backpropagation/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Introduction to Deep Learning",
            "description": "A series of notes for revision purpose",
            "published": "March 03, 2024",
            "authors": [
              
              {
                "author": "Albert Einstein",
                "authorURL": "https://en.wikipedia.org/wiki/Albert_Einstein",
                "affiliations": [
                  {
                    "name": "IAS, Princeton",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Boris Podolsky",
                "authorURL": "https://en.wikipedia.org/wiki/Boris_Podolsky",
                "affiliations": [
                  {
                    "name": "IAS, Princeton",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Nathan Rosen",
                "authorURL": "https://en.wikipedia.org/wiki/Nathan_Rosen",
                "affiliations": [
                  {
                    "name": "IAS, Princeton",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Long</span> Nguyen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/recipe/">recipe </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Introduction to Deep Learning</h1> <p>A series of notes for revision purpose</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#deep-learning-what-is-it">Deep Learning - what is it?</a> </div> <ul> <li> <a href="#interview-questions">Interview questions</a> </li> </ul> <div> <a href="#linear-classifier">Linear Classifier</a> </div> <ul> <li> <a href="#interview-questions">Interview questions</a> </li> </ul> <div> <a href="#multi-layer-perceptron">Multi-Layer Perceptron</a> </div> <ul> <li> <a href="#interview-questions">Interview questions</a> </li> </ul> <div> <a href="#backpropagation">Backpropagation</a> </div> <ul> <li> <a href="#interview-questions">Interview questions</a> </li> </ul> </nav> </d-contents> <p>This is the first in a series of blog notes dedicating to summarizing the most fundamental concepts in Machine Learning, Deep Learning, AI, and other scientific topics of my interest. Struggling to comprehend, distinguish, and reminisce these concepts myself sometimes, especially before and during important interviews, I would treasure a lot a source of simple, concise, accurate information to revise for those occasions. With that in mind, I created this blog series including fundamental AI/ML concepts and relating interview questions for those preparing to launch their career in the field, or for those who simply need a refresh before moving on to more complicated concepts. If you fancy to dig deeper into these topics, you can refer to my <a href="https://ottovintola.github.io/blog/" rel="external nofollow noopener" target="_blank">colleague’s blog</a> for a more comprehensive and thorough series.</p> <p>In this first blog, we will go through the concept of Deep Learning, Linear Classifier, Multi-Layer Perception (MLP) and Backpropagation</p> <h2 id="deep-learning---what-is-it">Deep Learning - what is it?</h2> <p><strong>Deep Learning (DL)</strong> is the subset of ML that focuses on the studies of Artificial Neural Networks (ANNs) with feature learning. DL architectures take in large amount of data (structured/unstructured), and use complex algorithms that utilizes multiple layers (thus “deep”) to progressively extract higher-level features and hidden patterns from the raw input. For example, lower layers might identify edges of an image while higher layers identify features that can easily be perceived by human, such as a dog, or a number.</p> <p><strong>Feature learning</strong> (supervised, unsupervised, self-supervised) or representation learning referes to the automatic extraction &amp; selection of relevant features to discover useful representations or patterns <em>directly</em> from raw input. This replaces the manual feature engineering/designing process and allows automatic learning and using the features for specific tasks.</p> <h3 id="interview-questions">Interview questions:</h3> <p><strong>Question: How does DL differ from traditional ML?</strong></p> <p><em>Answer: Unlike traditional ML, deep learning involves the use of deep neural networks with multiple layers (deep architectures) to automatically learn hierarchical representations of data. This allows deep learning models to discover intricate patterns and features of data, making them highly effective for tasks such as image or speech recognition.</em></p> <p><strong>Question: Explain the concept of NN layres and their role in DL</strong></p> <p><em>Answer: NN layers are the building blocks of DL models. They are responsible for processing and transforming input data. In a typical feedforward NN, there are 3 main types of layers: input, hidden and output. The input layer receives the raw data, hidden layers process this data through weighted connections and activation functions, and the output layer produces the final result. DL involves the use of multiple hidden layers, allowing the network to learn complex representations through hiearchical abstraction of features.</em></p> <p><strong>Question: How does training a DL model work? What is the role of loss functions?</strong></p> <p><em>Answer: Training a DL model involves presenting it with labeled training data, feeding it rhough the network, and adjusting the model’s parameters (weights and biases) to minimize a predefined loss function. The loss function quantifies the difference between the predicted output and the actual target values. During training, an optimization algorithm is used to iteratively update the model parameteres in the direction that reduces the loss. This continues until the model achieves satisfactory performace on the training data.</em></p> <p><strong>Question: Some common challenges and limitations associated with DL?</strong></p> <p><em>Answer: One of the biggest challenges is overfitting, where the model performs well on training data but poorly on new data. Other challenges include the need for a large labeled datasets, computational intensity, and interpretability as DL models are often considered black boxes. Additionally, transferability of learned knowledge across domains and the ethical considerations of biased models are ongoing concerns.</em></p> <p><em>Some consideration to mitigate these challenges: For overfitting, regularization techniques (placeholder for link) or using more intensive, diverse datasets for training and validation can contribute to a more generalized model. Techniques like model paralelism and distributed training across multiple devices can hlep manage computational requirements more efficiently. Pruning and quantization can reduce the size of the model (-&gt; lighter computation) without sacrificing too much performance. Leverage pre-trained models through transfer learning or data augmentation can help artificially increase training data size. Moreover, augmenting trainnig dataset with adversatial examples and robust optimization techniques can enhance the model’s resilience aggainst adversarial attacks.</em></p> <hr> <h2 id="linear-classifier">Linear Classifier</h2> <p><strong>Linear Classifier</strong> is the simplest ML models and thus it can be considered as elementary components for DL models, which is used for binary or multiclass classification tasks. It makes prediction by combining the input features with weights, and the decision boundary is a linear combination of these features. Therefore, its effectiveness depends on the linearity of the underlying patterns.</p> <hr> <h2 id="multi-layer-perceptron">Multi-Layer Perceptron</h2> <p><strong>MLP</strong> is a supplement of feedforward neural network that consists of 3 types of layers: 1 input, 1 or several hidden (because the outputs of this layer are not visible outside the model ), and 1 output layer. Each node in a layer is connected to every node in the next, thus these layers are called <em>fully-connected</em>. MLP can be applied to various tasks, from classification, regression, and pattern recognition</p> <hr> <h2 id="backpropagation">Backpropagation</h2> <p><strong>Backpropagation</strong> is a gradient estimation used to train NN models. The key idea is to iteratively adjust the weights of the NN to reduce the error between the predicted and actual outputs. Back prob consists of <em>two</em> phases/passes:</p> <ul> <li> <p>Forward pass: Input is fed into the NN, and the weighted sum of inputs is calculated at each neuron in each layer. Activation functions are applied to these sums to produce the output of each neuron. Then, the error or loss is calculated by comparing this output to the actual target values.</p> </li> <li> <p>Backward pass: This pass involves the propagation of the error backward through the network to update the weights. The gradient of the loss function with respect to the weights is calculated using the chain rule of calculus. The weights are then adjusted in the opposite direction of the gradient to minimize the error, using done using optimization algorithm such as gradient descent. The updated weights improve the network’s performance, and the process is repeated for multiple epochs until the model converges to a satisfactory solution.</p> </li> </ul> <h3 id="interview-questions-1">Interview questions</h3> <p><strong>Question: Explain the steps involved in the backpropagation algorithm and how it works in training a neural network:</strong></p> <p><em>Answer: refer to the 2 passes above</em></p> <p><strong>Question: What is the role of the activation function in the backpropagation algorithm? Examples of commonly used activation functions and explain when to use them?</strong></p> <p><em>Answer: Activation function introduces non-linearity into the NN, allowing it to learn complex patterns. Common activation functions include:</em></p> <ul> <li>Rectified Linear Unit (ReLU) for hidden layers due to its simplicity and ability to mitigate the vanishing gradient problem</li> <li>Sigmoid for inary classification output</li> <li>Softmax for multi-class classification output to provide probability distribution over the classes</li> </ul> <p><strong>Question: How does the vanishing gradient problem affect the training of deep neural networks, and what strategies can be employed to mitigate this issue in the context of backpropagation?</strong></p> <p><em>Answer: Vanishing gradient problem occurs when gradients become extremely small during backpropagation as more layers using the same activation functions are added to the network, hindering the update of weights in earlier layers. This is because, by the chain rule, the derivatives of each layer are multiplied together (backward); thus the gradient decreases exponentially as we back-propagate to the initial layers. This is particularly problematic in deep NN.</em></p> <p><em>To mitigate this, we could use weight initialization methods such as He for ReLU which doesn’t cause a small derivative. Alternatively, we could use non-saturating activation functions, or implement skip connections (e.g. in Residual Networks).</em></p> <p><strong>Question: Impact of hyperparameters (learning rate, batch size, number of epochs) on the backpropagation algorithm?</strong></p> <p><em>Answer: Learning rate determines the step size during weight updates - too high rate might lead to overshooting while too low rate leads to slow convergence. Batch size affects the efficiency of weight updates - larger batches provide smoother convergence. The number of epochs influences the duration of training. Setting these hyperparameters involves trade-offs between dataset size, computational resources, and the need for regularization.</em></p> <p><strong>Question:Explain the concept of weight initialization in the context of backpropagation.</strong></p> <p><em>Answer: Weight initialization is crucial as it influences the starting point of the optimization process. Poor initialization can lead to slow convergence or getting stuck in local minima. Common techniques include He initialization for ReLU and Xavier/Glorot initialization for Sigmoid or Tanh. These methods set the initial weights that balance the variance of inputs and outputs in each layer, promoting a stable and effective training process</em></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Long Nguyen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>